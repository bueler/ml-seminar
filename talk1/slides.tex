% Copyright 2021  Ed Bueler

\documentclass[xcolor={svgnames},
               hyperref={colorlinks,citecolor=DeepPink4,linkcolor=FireBrick,urlcolor=Maroon}]
               {beamer}

\mode<presentation>{
  \usetheme{Madrid}
  \usecolortheme{seagull}
  \setbeamercovered{transparent}
  \setbeamerfont{frametitle}{size=\large}
}

\setbeamercolor*{block title}{bg=red!10}
\setbeamercolor*{block body}{bg=red!5}

\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{times}
\usepackage[T1]{fontenc}
% Or whatever. Note that the encoding and the font should match. If T1
% does not look nice, try deleting the line with the fontenc.

\usepackage{empheq}
\usepackage{xspace}
\usepackage{verbatim,fancyvrb}

\usepackage{tikz}
\usetikzlibrary{shapes,arrows.meta,decorations.markings,decorations.pathreplacing,fadings,positioning}

\usepackage[kw]{pseudo}
\pseudoset{left-margin=15mm,topsep=5mm,idfont=\texttt}


% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command:
%\beamerdefaultoverlayspecification{<+->}

\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bbf}{\mathbf{f}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bn}{\mathbf{n}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bw}{\mathbf{w}}

\newcommand{\bF}{\mathbf{F}}
\newcommand{\bG}{\mathbf{G}}
\newcommand{\bQ}{\mathbf{Q}}

\newcommand{\grad}{\nabla}
\newcommand{\Div}{\nabla\cdot}
\newcommand{\minmod}{\operatorname{minmod}}

\newcommand{\CC}{\mathbb{C}}
\newcommand{\RR}{\mathbb{R}}

\newcommand{\ddt}[1]{\ensuremath{\frac{\partial #1}{\partial t}}}
\newcommand{\ddx}[1]{\ensuremath{\frac{\partial #1}{\partial x}}}
\newcommand{\Matlab}{\textsc{Matlab}\xspace}
\newcommand{\Octave}{\textsc{Octave}\xspace}
\newcommand{\eps}{\epsilon}

\newcommand{\ip}[2]{\left<#1,#2\right>}

\newcommand{\xiphalf}{{x_{i+\frac{1}{2}}}}
\newcommand{\ximhalf}{{x_{i-\frac{1}{2}}}}
\newcommand{\Fiphalf}{{F_{i+\frac{1}{2}}}}
\newcommand{\Fimhalf}{{F_{i-\frac{1}{2}}}}
\newcommand{\Fiphalfn}{{F^n_{i+\frac{1}{2}}}}
\newcommand{\Fimhalfn}{{F^n_{i-\frac{1}{2}}}}

\newcommand{\trefcolumn}[1]{\begin{bmatrix} \phantom{x} \\ #1 \\ \phantom{x} \end{bmatrix}}
\newcommand{\trefmatrixtwo}[2]{\left[\begin{array}{c|c|c} & & \\ #1 & \dots & #2 \\ & & \end{array}\right]}
\newcommand{\trefmatrixthree}[3]{\left[\begin{array}{c|c|c|c} & & & \\ #1 & #2 & \dots & #3 \\ & & & \end{array}\right]}
\newcommand{\trefmatrixgroups}[4]{\left[\begin{array}{c|c|c|c|c|c} & & & & & \\ #1 & \dots & #2 & #3 & \dots & #4 \\ & & & & & \end{array}\right]}

\newcommand{\blocktwo}[4]{\left[\begin{array}{c|c} #1 & #2 \\ \hline #3 & #4 \end{array}\right]}

\newcommand{\bqed}{{\color{blue}\qed}}
\newcommand{\ds}{\displaystyle}

\newcommand\mynum[1]{{\renewcommand{\insertenumlabel}{#1}%
      \usebeamertemplate{enumerate item} \,}}


\title{Getting started with machine learning}

\subtitle{({one little artificial neural net})}

\author{Ed Bueler}

\institute[UAF]{MATH 692 Mathematics for Machine Learning \\ UAF}

\date[Spring 2022]{13 January 2022}

%\titlegraphic{\begin{picture}(0,0)
%    \put(0,180){\makebox(0,0)[rt]{\includegraphics[width=4cm]{figs/software.png}}}
%  \end{picture}
%}

%% this nonsense needed to start section counter at 0; see
%% https://tex.stackexchange.com/questions/170222/change-the-numbering-in-beamers-table-of-content
\makeatletter
\patchcmd{\beamer@sectionintoc}
  {\ifnum\beamer@tempcount>0}
  {\ifnum\beamer@tempcount>-1}
  {}
  {}
%\beamer@tocsectionnumber=-1
\makeatother


\begin{document}
\beamertemplatenavigationsymbolsempty

\begin{frame}
  \maketitle
\end{frame}

\begin{frame}{today's talk}

\begin{itemize}
\item \alert{my topic:} {\small how this $\downarrow$ neural network does this $\downarrow$ classification task}

\medskip
\hspace{5mm} \includegraphics[height=25mm]{figs/network.png} \hfill \includegraphics[height=23mm]{figs/classification} \hspace{10mm}

\medskip
\item an \alert{example} from this $\downarrow$ paper:

\medskip

HH19 \, $=$ \, 
\begin{minipage}[t]{0.75\textwidth} \footnotesize
C.~F.~Higham \& D.~J.~Higham (2019). \href{http://www.math.stonybrook.edu/~bishop/classes/math533.S21/MachineLearning/SIAMreview.pdf}{\emph{Deep learning: An introduction for applied mathematicians.}} SIAM Review, 61(4), 860-891
\end{minipage}
\end{itemize}
\end{frame}


\begin{frame}{today's talk}

\begin{itemize}
\item \alert{goal for today:} know the meanings of

\medskip
\small
\qquad \begin{tabular}{ll}
\emph{artificial neuron} \qquad & \emph{activation function} \\
\emph{weight matrix} & \emph{bias vector} \\
\emph{back-propagation} & \emph{stochastic gradient method}
\end{tabular}

\medskip
\item which standard mathematical concept(s) match these buzzwords?
\end{itemize}
\end{frame}


\begin{frame}{big caveat}

\begin{itemize}
\item I am no expert on what I am talking about here
\item many in room know more than me
\item I volunteered to give one intro talk, that's all!
\end{itemize}
\end{frame}


\begin{frame}{\emph{participant}-driven seminar logistics}

\begin{itemize}
\item sign-up sheet!
\item my existing webpages:
    \begin{itemize}
    \item[$\circ$] \href{http://bueler.github.io/M692S22/index.html}{\texttt{bueler.github.io/M692S22}}
    \item[$\circ$] \href{https://github.com/bueler/ml-seminar}{\texttt{github.com/bueler/ml-seminar}}
    \end{itemize}
\item in-person or hybrid?
    \begin{itemize}
    \item[$\circ$] is this classroom adequate?
    \end{itemize}
\item what will be the topics?
    \begin{itemize}
    \item[$\circ$] are there out-of-bounds topics?
    \item[$\circ$] who is volunteering to talk?
    \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{Outline}
  \tableofcontents[hideallsubsections]
\end{frame}

\section{a single artificial neuron}

\begin{frame}{one artificial neuron $=$ nonlinear-ized inner product}

\begin{columns}
\begin{column}{0.55\textwidth}
\begin{itemize}
\item given (column) vectors $v,w \in \RR^n$
\item recall \emph{inner product}:
    $$\ip{{\color{blue} w}}{v} = {\color{blue} w}^\top v = \sum_{j=1}^n {\color{blue} w_j} v_j$$
\item apply a nonlinear function $\sigma: \RR^1 \to \RR^1$:
\only<1>{
\begin{equation*}
{\color{red} a} = \sigma\left(\sum_{j=1}^n {\color{blue} w_j} v_j\right) \in \RR^1
\end{equation*}
}
\only<2>{
\begin{equation*}
{\color{red} a} = \sigma\left(\sum_{j=1}^n {\color{blue} w_j} v_j + {\color{ForestGreen} b}\right) \in \RR^1
\end{equation*}
}
    \begin{itemize}
    \item<2>[$\circ$] detail: add a bias ${\color{ForestGreen} b} \in \RR^1$
    \item<2>[$\circ$] that's it! an \alert{artificial neuron}
    \end{itemize}
\end{itemize}
\end{column}
\begin{column}{0.45\textwidth}
\only<1>{\includegraphics[width=\textwidth]{figs/single-neuron}}
\only<2>{\includegraphics[width=\textwidth]{figs/b-single-neuron}}
\end{column}
\end{columns}
\end{frame}


\begin{frame}{neuron roles}

\begin{columns}
\begin{column}{0.55\textwidth}
\begin{itemize}
\item $v$ is input
\item \alert{weights} ${\color{blue} w}$ and \alert{biases} ${\color{ForestGreen} b}$ are parameters
    \begin{itemize}
    \item[$\circ$] they need \alert{training}
    \end{itemize}
\item the \alert{activation function} $\sigma$ is fixed
\item the output ${\color{red} a}$ is the \alert{activation} of the neuron
\end{itemize}
\end{column}
\begin{column}{0.45\textwidth}
\includegraphics[width=\textwidth]{figs/b-single-neuron}
\end{column}
\end{columns}
\end{frame}


\begin{frame}{nonlinear activation function}

\begin{center}
\includegraphics[height=30mm]{figs/sigmoid} \hspace{10mm} \includegraphics[height=30mm]{figs/relu}
\end{center}

\begin{itemize}
\item $\sigma$ is the \alert{activation function}
\item possibilities:
    \begin{itemize}
    \item[$\circ$] \alert{sigmoid}, e.g.\qquad $\displaystyle \sigma(z) = \frac{1}{1 + e^{-z}}$ \quad (left)
    \item[$\circ$] \alert{rectified linear unit (ReLU)},\qquad $\displaystyle \sigma(z) = \begin{cases} z, & z > 0 \\ 0, & z \le 0 \end{cases}$ \quad (right)
    \end{itemize}
\item $\sigma$ is increasing, with bounded derivative
\end{itemize}
\end{frame}


\begin{frame}{a trained neuron}

\begin{columns}
\begin{column}{0.55\textwidth}
\begin{itemize}
\item a \alert{trained neuron} is one with known parameters $w,b$
\item so $a:\RR^n \to \RR^1$ is a known function:
    $${\color{red} a} = \sigma(v)$$

    \begin{itemize}
    \item[$\circ$] similar cost to inner product
    \item[$\circ$] backward stable
    \end{itemize}
\item one might write
    $${\color{red} a} = \sigma(v;{\color{blue} w},{\color{ForestGreen} b})$$
to make dependence on parameters clear
\end{itemize}
\end{column}
\begin{column}{0.45\textwidth}
\includegraphics[width=\textwidth]{figs/b-single-neuron}
\end{column}
\end{columns}
\end{frame}


\section{forward through a neural net}

\begin{frame}{Outline}
  \tableofcontents[hideallsubsections,currentsection]
\end{frame}


\begin{frame}{feed-forward networks}

\begin{center}
\includegraphics[height=30mm]{figs/network}
\end{center}

\begin{itemize}
\item assumption made here: considering only \alert{feed-forward} networks
    \begin{itemize}
    \item[$\circ$] edges only connect consecutive layers, in order
    \item[$\circ$] in language of graph theory: connected, directed acyclic graph (DAG) which is equal to its own transitive reduction \dots I think
    \item[$\circ$] \alert{feed-forward} versus \alert{recurrent}
    \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{network notation}

\begin{center}
\includegraphics[height=35mm]{figs/state-notation}
\end{center}

\begin{itemize}
\item notation from HH19
\item ${\color{red} n_\ell}$ is number of neurons in layer $\ell=1,\dots,L$
    \begin{itemize}
    \item[$\circ$] $\ell=1$ is \alert{input} layer
    \item[$\circ$] input values equal first-layer activations: $x = {\color{blue} a^{[1]}} \in \RR^{{\color{red} n_1}}$)
    \item[$\circ$] $\ell=L$ is \alert{output} layer
    \item[$\circ$] output values equal $L$th-layer activations: $y = {\color{blue} a^{[L]}} \in \RR^{{\color{red} n_L}}$)
    \end{itemize}
\item activations in layer $\ell$ form a vector ${\color{blue} a^{[\ell]}} \in \RR^{{\color{red} n_\ell}}$
    \begin{itemize}
    \item[$\circ$] ${\color{blue} a_j^{[\ell]}}$ is activation of neuron $j$ in layer $\ell$
    \item[$\circ$] we label a neuron by its activation: ``neuron ${\color{blue} a_j^{[\ell]}}$''
    \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{weight notation}

\begin{center}
\includegraphics[height=40mm]{figs/weight-notation}
\end{center}

\begin{itemize}
\item weight ${ \color{ForestGreen} w_{jk}^{[\ell]} }$ on the edge from neuron ${\color{blue} a_k^{[\ell-1]}}$ to neuron ${\color{blue} a_j^{[\ell]}}$
\item thus
    $${\color{blue} a_j^{[\ell]}} = \sigma\left(\sum_{k=1}^{n_{\ell-1}} {\color{ForestGreen} w_{jk}^{[\ell]}} {\color{blue} a_k^{[\ell-1]}} + b_j\right)$$
\item which suggests matrix-vector multiplication!
\end{itemize}
\end{frame}


\begin{frame}{weight notation using vectors and matrices}

\begin{center}
\includegraphics[height=35mm]{figs/weight-notation}
\end{center}

\begin{itemize}
\item one row vector ${\color{ForestGreen} w^\top}$ of weights for each neuron
\item neurons in a layer deserve a matrix:
    $${\color{ForestGreen} W^{[\ell]}} = \left(\text{weights into layer $\ell$}\right) = \begin{bmatrix} {\color{ForestGreen} w_{jk}^{[\ell]}} \end{bmatrix}$$

    \begin{itemize}
    \item[$\circ$] ${\color{ForestGreen} W^{[\ell]}}$ is $n_{\ell}\times n_{\ell-1}$
    \end{itemize}
\item assume $\sigma$ applied entrywise, thus with biases $b^{[\ell]} \in \RR^{n_{\ell}}$,
    $${\color{blue} a^{[\ell]}} = \sigma\left({\color{ForestGreen} W^{[\ell]}} {\color{blue} a^{[\ell-1]}} + b^{[\ell]}\right)$$
\end{itemize}
\end{frame}


\begin{frame}{forward pass = nonlinear-ized matrix multiplication}

\begin{center}
\includegraphics[height=25mm]{figs/network}
\end{center}

\begin{itemize}
\item for this small $L=4$ network:
\begin{align*}
y = a^{[4]} &= \sigma\left(W^{[4]} a^{[3]} + b^{[4]}\right) = \dots \\
            &= \sigma\left(W^{[4]} \sigma\left(W^{[3]} \sigma\left(W^{[2]} a^{[1]} + b^{[2]}\right) + b^{[3]}\right) + b^{[4]}\right) \\
            &= \sigma\left(W^{[4]} \sigma\left(W^{[3]} \sigma\left(W^{[2]} x + b^{[2]}\right) + b^{[3]}\right) + b^{[4]}\right)
\end{align*}
\item if $\sigma(z)=z$ and $b^{[\ell]}=0$ then $y = W^{[4]} W^{[3]} W^{[2]} x$
\item feed-forward network = nonlinear \& affine matrix multiplication
\end{itemize}
\end{frame}


\begin{frame}{general forward-pass neural network formulas}

\begin{itemize}
\item the general formulas are equations (3.1), (3.2) in HH19
\item we act on an input vector $x$ to give an output vector $y$:
\begin{align*}
a^{[1]} &= x \in \RR^{n_1} \\
a^{[\ell]} &= \sigma\left(W^{[\ell]} a^{[\ell-1]} + b^{[\ell]}\right) \in \RR^{n_\ell} \qquad \text{for } \ell=2,3,\dots,L \\
y &= a^{[L]}
\end{align*}
\item a forward pass is an obvious \texttt{for} loop, given
    \begin{itemize}
    \item[$\circ$] weights $\{W^{[\ell]}\}$ and biases $\{b^{[\ell]}\}$ stored in some data structure
    \item[$\circ$] $\sigma(z)$ implemented as a \texttt{function}
    \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{forward-pass computation work model (\emph{minor point})}

\begin{itemize}
\item \alert{work} $=$ number of floating-point operations
\item work at layer $\ell$:
    $$2 n^{[\ell-1]} n^{[\ell]} + O(n^{[\ell]}) = O(n^{[\ell-1]} n^{[\ell]})$$

    \begin{itemize}
    \item[$\circ$] evaluating activation functions is cheap
    \item[$\circ$] forward-pass work is basically just matrix multiplications
    \end{itemize}
\item let $n_{\text{max}} =\max\{n^{[\ell]}\}$, the maximum layer size
\item total forward-pass work:
    $$\sum_{\ell=2}^L O(n^{[\ell-1]} n^{[\ell]}) = O(L n_{\text{max}}^{\,2})$$

    \begin{itemize}
    \item[$\circ$] using big-O in the ``$n_{\text{max}} \to \infty$'' limit of big layers
    \item[$\circ$] asymptotically same cost as $L$ matrix-vector products
    \item[$\circ$] easily computed by GPU hardware
        \begin{itemize}
        \item \emph{versus} solving linear systems \dots
        \end{itemize}
    \end{itemize}
\end{itemize}
\end{frame}


\section{training is merely optimization}

\begin{frame}{Outline}
  \tableofcontents[hideallsubsections,currentsection]
\end{frame}


\begin{frame}{training is a biologically-motivated procedure}

\begin{itemize}
\item imagine teaching your dog to read numbers 1,2,3:
\begin{center}
\includegraphics[width=0.7\textwidth]{figs/onetwothree}
\end{center}
\item example training procedure:
    \begin{enumerate}
    \item randomly present one image of a digit from above
\begin{center}
\includegraphics[width=0.1\textwidth]{figs/two} \phantom{dlasbj sadkf adsk}
\end{center}
    \item if dog barks correct number of times then gets treat

\vspace{5mm}
\hspace{10mm} \emph{bark! bark!} \qquad $\implies$

\vspace{-6mm}
\hfill \includegraphics[width=0.15\textwidth]{figs/dogtreat} \hspace{25mm} \phantom{boo}
    \item otherwise move on to next image
    \end{enumerate}
\end{itemize}
\end{frame}


\begin{frame}{training is a biologically-motivated procedure}

\begin{itemize}
\item biological learning can last from hours to decades
\item something permanent changes within the brain
    \begin{itemize}
    \item[$\circ$] neuron excitation is electrical (activation is temporary)
    \item[$\circ$] neuron count is relatively fixed
    \item[$\circ$] in learning, there are chemical or morpological changes in connections between neurons, the \alert{synapses} where an axon connects to another neuron's dendrite
    \end{itemize}
\end{itemize}

\begin{center}
\mbox{\includegraphics[width=0.5\textwidth]{figs/physicalneuron} \includegraphics[width=0.4\textwidth]{figs/synapse}}
\end{center}
\end{frame}


\begin{frame}{artificial neurons as a model of real neurons}

\begin{itemize}
\item biological realism is \emph{not} needed for machine learning
\item replace with a simplified artificial neuron:

\begin{center}
\includegraphics[width=0.3\textwidth]{figs/b-single-neuron}
\end{center}

    \begin{itemize}
    \item[$\circ$] the model is merely a formula: \quad $\displaystyle {\color{red} a} = \sigma\left(\sum_{k=1}^{n} {\color{blue} w_{k}} v_k + {\color{ForestGreen} b}\right)$
    \end{itemize}
\item training will make ``permanent'' changes in the weights ${\color{blue} w_k}$ and biases ${\color{ForestGreen} b}$
\item after training, forward passes are the network's ``learned behavior''
\end{itemize}
\end{frame}


\begin{frame}{how does training work?}

\begin{itemize}
\item so, how does training work in artificial neural networks?
\item only considering \alert{supervised training} here
\item given: $N$ pieces of \alert{labeled data} (pairs)
    $$(x^{\{i\}}, y^{\{i\}}) \qquad \text{for } i=1,\dots,N$$

    \begin{itemize}
    \item[$\circ$] $x^{\{i\}} \in \RR^{n_1}$ are the \alert{data}
    \item[$\circ$] $y^{\{i\}} \in \RR^{n_L}$ are the \alert{labels}
        \begin{itemize}
        \item note that the labeled data determine the number of neurons in the first and last layers
        \end{itemize}
    \item[$\circ$] in a \alert{classification task}, the $y^{\{i\}}$ only take on finitely-many pre-specified values
        \begin{itemize}
        \item e.g.~$y^{\{i\}} = [1,\, 0]^\top$ and $y^{\{i\}} = [0,\, 1]^\top$ are the only possibilities in the classification task later
        \end{itemize}
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{how does training work?}

\begin{itemize}
\item \alert{supervised training} means choosing the weights $W^{[\ell]}$ and biases $b^{[\ell]}$, in \emph{all} the layers, so as to approximately minimize the \alert{average misfit} between the the network output for the each data vector and the corresponding (\alert{correct}) label vector
\item using the squared $2$-norm for the misfit, this is a formula:
    $$\text{Cost} = \frac{1}{N} \sum_{i=1}^N \frac{1}{2} \|y^{\{i\}} - a^{[L]}(x^{\{i\}})\|_2^2$$

    \begin{itemize}
    \item[$\circ$] $a^{[L]}(x^{\{i\}})$ denotes the output-layer activation from a forward pass with input $x^{\{i\}}$
    \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{better training notation}

\begin{itemize}
\item let's give all the parameters a single-letter name:
   $$p = \{W^{[2]},W^{[3]},\dots,W^{[L]},b^{[2]},b^{[3]},\dots,b^{[L]}\} \in \RR^s$$

    \begin{itemize}
    \item[$\circ$] $p$ collects all weight matrices and biases into one big column vector
    \end{itemize}
\item define the cost (misfit) of the network for one data pair:
    $$C^{\{i\}}(p) = \frac{1}{2} \left\|y^{\{i\}} - a^{[L]}(x^{\{i\}})\right\|_2^2$$
\item let $C(p)$ be the total \alert{cost functional}, or \alert{objective}, namely the \alert{average} misfit over all the labeled data:
\begin{align*}
C(p) &= \frac{1}{N} \sum_{i=1}^N C^{\{i\}}(p) \\
     &= \frac{1}{2N} \sum_{i=1}^N \left\|y^{\{i\}} - a^{[L]}(x^{\{i\}})\right\|_2^2
\end{align*}
\end{itemize}
\end{frame}


\begin{frame}{training $=$ nonlinear least-squares optimization}

\begin{itemize}
\item compare
    $$C(p) = \frac{1}{2N} \sum_{i=1}^N \left\|y^{\{i\}} - a^{[L]}(x^{\{i\}})\right\|_2^2$$
to ``nonlinear least-squares'' in a standard optimization textbook (Nocedal \& Wright, 2006; Chapter 10):

\medskip
\includegraphics[width=0.9\textwidth]{figs/nls}

\medskip
\item training a neural net is \alert{nonlinear least-squares optimization}
    \begin{itemize}
    \item[$\circ$] $\left\|y^{\{i\}} - a^{[L]}(x^{\{i\}})\right\|_2$ is the \alert{residual norm} for $i$th data
    \item[$\circ$] it becomes zero when the network fully-learns the $i$th data
    \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{fundamental idea: cost is a function of weights and biases}

\begin{itemize}
\item recall:
   $$p = \{W^{[2]},W^{[3]},\dots,W^{[L]},b^{[2]},b^{[3]},\dots,b^{[L]}\}$$
collects all weight matrices and biases into one big vector
\item \alert{the cost for one data pair is a function of} $p$:
    $$C^{\{i\}}(p) = \frac{1}{2} \left\|y^{\{i\}} - a^{[L]}(x^{\{i\}})\right\|_2^2$$
\item that is, always remember:

\bigskip
\begin{quote}
given parameters $p$, one forward pass through the network is needed to evaluate $C^{\{i\}}(p)$
\end{quote}

\medskip
    \begin{itemize}
    \item[$\circ$] forward pass uses input $x^{\{i\}}$
    \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{gradient descent}

\begin{itemize}
\item our goal is to minimize \quad \small $\displaystyle C(p) = \frac{1}{2N} \sum_{i=1}^N \left\|y^{\{i\}} - a^{[L]}(x^{\{i\}})\right\|_2^2$ \normalsize
\item the function $C(p)$ is differentiable
    \begin{itemize}
    \item[$\circ$] \emph{why differentiable? what does this assume about the network?}
    \end{itemize}
\item \dots thus we can compute the \alert{gradient} $\grad C(p)$
\item the gradient points \emph{up hill} on the surface $C:\RR^s\to\RR$,
\item natural idea: do \alert{gradient descent}
\end{itemize}
\begin{pseudo*}
\pr{gd}(p)\text{:} \\+
    for $i = 1,2,\dots$ \\+
        $p = p - \eta \grad C(p)$ \\-
    return $p$
\end{pseudo*}

\vspace{-25mm}
\hfill \includegraphics[width=0.45\textwidth]{figs/gdsurface}
\end{frame}


\begin{frame}{gradient descent is miserable}

\begin{pseudo*}
\pr{gd}(p)\text{:} \\+
    for $i = 1,2,\dots$ \\+
        $p = p - \eta \grad C(p)$ \\-
    return $p$
\end{pseudo*}

\vspace{-25mm}
\hfill \includegraphics[width=0.35\textwidth]{figs/gdsurface} \phantom{adslj}

\medskip
\begin{itemize}
\item foo
\end{itemize}
\end{frame}


\begin{frame}{buzzword summary (so far)}

\begin{itemize}
\item \alert{artificial neuron}
    \begin{itemize}
    \item[$\circ$] \alert{activation}
    \item[$\circ$] \alert{activation function}
        \begin{itemize}
        \item sigmoid, ReLU
        \end{itemize}
    \item[$\circ$] \alert{weight}
    \item[$\circ$] \alert{bias}
    \end{itemize}
\item \alert{artificial neural network} = ANN
    \begin{itemize}
    \item[$\circ$] feed-forward network
    \end{itemize}
\item \alert{training}
    \begin{itemize}
    \item[$\circ$] \alert{supervised learning}
    \item[$\circ$] labeled data
    \item[$\circ$] nonlinear least-squares optimization
    \end{itemize}
\item \alert{stochastic gradient descent}

\bigskip
\item \alert{machine learning}
    \begin{itemize}
    \item[$\circ$] \alert{deep learning} if $L>2$
    \end{itemize}
\end{itemize}
\end{frame}


\section{backward through a neural net}

\begin{frame}{Outline}
  \tableofcontents[hideallsubsections,currentsection]
\end{frame}


\begin{frame}{fixme}

\begin{itemize}
\item latex
\item beamer
\end{itemize}
\end{frame}


\section{running the codes yourself}

\begin{frame}{Outline}
  \tableofcontents[hideallsubsections,currentsection]
\end{frame}


\begin{frame}{\emph{Matlab online} instructions: HH19 codes}

\begin{itemize}
\item as a UAF person you have access to Matlab online if you want it

\begin{center}
\href{https://matlab.mathworks.com/}{\texttt{matlab.mathworks.com}}
\end{center}

    \begin{itemize}
    \item[$\circ$] I also use Octave
    \end{itemize}
\item 
\item beamer
\end{itemize}
\end{frame}


\begin{frame}{\emph{Matlab online} instructions: my versions of HH19 codes}

\begin{itemize}
\item FIXME
\item beamer
\end{itemize}
\end{frame}


\section{things that bother me}

\begin{frame}{Outline}
  \tableofcontents[hideallsubsections,currentsection]
\end{frame}


\begin{frame}{fixme}

\begin{itemize}
\item latex
\item beamer
\end{itemize}
\end{frame}


\begin{frame}{future topics?}

\begin{itemize}
\item CNN
\item GNN
\item Newton and Quasi-Newton optimization methods
    \begin{itemize}
    \item[$\circ$] L-BFGS
    \end{itemize}
\item stochastic optimization
    \begin{itemize}
    \item[$\circ$] online machine learning
    \item[$\circ$] Adam
    \end{itemize}
\end{itemize}
\end{frame}

\end{document}
